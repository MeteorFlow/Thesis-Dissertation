\subsection{Traditional Database Systems}
Database systems perform vital functions for all sorts of organizations because
of the growing importance of using and managing data efficiently. A database
system consists of a software, a database management system (DBMS) and one or
several databases. DBMS is a set of programs that enables users to store, manage
and access data. In other words, the database is processed by DBMS, which runs
in the main memory and is controlled by the respective operating system.

A database is a logically coherent collection of data with some inherent meaning
and represents some aspects of the real world. A random assortment of data
cannot be referred to as a database. Databases draw a sharp distinction between
data and information. Data are known facts that can be recorded and that have
implicit meaning. Information is data that has been organized and prepared in a
form that is suitable for decision-making. Shortly information is the analysis
and synthesis of data. The most fundamental terms used in the database approach
are identity, attributes and relationships. An entity is something that can be
identified in the user's work environment, something that the users want to
track. It may be an object with a physical or conceptual existence. An attribute
is a property of an entity. A particular entity will have a value for each of
its attributes. The attribute values that describe each entity become a major
part of the data stored in the database.

Database Management System is a general-purpose software system designed to
manage large bodies of information facilitating the process of defining,
constructing and manipulating databases for various applications. Specifying
data types, structures and constraints for the data to be stored in the database
is called defining a database. Constructing the database is the process of
storing data itself on some storage medium that is controlled by the DBMS.
Querying to retrieve specific data, updating the database to reflect changes and
generating reports from the data is the main concept of manipulating a database.
The DBMS functions as an interface between the users and the database ensuring
that the data is stored persistently over long periods, independent of the
programs that access it \cite{latisen1998}. DBMS can be divided into three
subsystems; the design tools subsystem, the run-time subsystem and the DBMS
engine.

The design tools subsystem has a set of tools to facilitate the design and
creation of the database and its applications. Tools for creating tables, forms,
queries and reports are components of this system. DBMS products also provide
programming languages and interfaces to programming languages. The run-time
subsystem processes the application components that are developed using the
design tools. The last component of DBMS is the DBMS engine which receives
requests from the other two components and translates those requests into
commands to the operating system to read and write data on physical media
\cite{elmasri1998}.

The database approach has several advantages over traditional file processing in
which each user has to create and define files needed for a specific
application. In these systems' duplication of data is generally inevitable
causing wasted storage space and redundant efforts to maintain common data
up-to-date. In the database approach data is maintained in a single storage
medium and accessed by various users. The self-describing nature of database
systems provides information not only about the database itself but also about
the database structure such as the type and format of the data. A complete
definition and description of database structure and constraints, called
meta-data, is stored in the system catalog. Data abstraction is a consequence of
this self-describing nature of database systems allowing program and data
independence. DBMS access programs do not require changes when the structure of
the data files is changed hence the description of data is not embedded in the
access programs. This property is called program-data independence. Support of
multiple views of data is another important feature of database systems, which
enables different users to view different perspectives of databases depending on
their requirements. In a multi-user database environment, users probably have
access to the same data at the same time as well as they can access different
portions of the database for modification. Concurrency control is crucial for a
DBMS so that the results of the updates are correct. The DBMS software ensures
that concurrent transactions operate correctly when several users are trying to
update the same data.

Using a DBMS also eliminates unnecessary data redundancy. In the database
approach, each primary fact is generally recorded in only one place in the
database. Sometimes it is desirable to include some limited redundancy to
improve the performance of queries when it is more efficient to retrieve data
from a single file instead of searching and collecting data from several files,
but this data duplication is controlled by DBMS to prohibit inconsistencies
among files. By eliminating data redundancy inconsistencies among data are also
reduced \cite{elmasri1998}. Reducing redundancy improves the consistency of data
while reducing the waste in storage space. DBMS allows data sharing to the
users. Sharing data often permits new data processing applications to be
developed without having to create new data files. In general, less redundancy
and greater sharing lead to less confusion between organizational units and less
time spent resolving errors and inconsistencies in reports. The database
approach also permits security restrictions. In a DBMS different types of
authorizations are accepted to regulate which parts of the database various
users can access or update.

\subsection{A Modern Approach to Data Systems}
In contemporary computing environments, the dominance has shifted towards
data-intensive applications, deviating from the traditional emphasis on
compute-intensive tasks. The limiting factor for these applications seldom
resides in the sheer computational power of the CPU; rather, the primary
challenges typically revolve around the magnitude of the data, its intricate
structures, and the rapidity with which it changes. Unlike compute-intensive
operations that heavily rely on processing speed, data-intensive applications,
dealing with extensive datasets, intricate data structures, or swiftly evolving
information, necessitate adept strategies for storage, retrieval, and
manipulation. Consequently, effectively addressing the multifaceted dynamics of
data becomes paramount, highlighting the imperative for sophisticated data
management and processing techniques to optimize performance in the face of
these intricate challenges.

Why should we amalgamate these diverse elements within the overarching label of
data systems? Recent years have witnessed the emergence of a plethora of novel
tools for data storage and processing, each meticulously optimized for an array
of distinct use cases \cite{stonebraker2005onesize}. Consider, for instance,
datastores that concurrently function as message queues (e.g., Redis) or message
queues equipped with database-like durability assurances (such as Apache Kafka).
The demarcation lines between these categories are progressively fading,
reflecting a landscape where boundaries are increasingly ambiguous.

Moreover, a growing number of applications now present challenges of such
magnitude or diversity that a solitary tool is no longer sufficient to fulfill
all its data processing and storage requisites. Instead, the workload is
deconstructed into tasks amenable to efficient execution by individual tools.
These disparate tools are then intricately interwoven using application code,
offering a nuanced and adaptable approach to the multifaceted demands of
contemporary data management and processing.

In navigating the complex landscape of application development, the quest for
reliability, scalability, and maintainability unveils a challenging yet
essential journey. As we delve into the intricate patterns and techniques that
permeate various applications, we embark on an exploration to fortify these
foundational pillars in the realm of software systems.

Ensuring reliability involves the meticulous task of ensuring systems function
correctly, even when confronted with faults. These faults may manifest in the
hardware domain as random and uncorrelated issues, in software as systematic
bugs that are challenging to address, and inevitably in humans who occasionally
err. Employing fault-tolerance techniques becomes imperative to shield end users
from specific fault types.

Scalability, on the other hand, necessitates the formulation of strategies to
maintain optimal performance, particularly when facing heightened loads. To
delve into scalability, it becomes essential to establish quantitative methods
for describing load and performance. An illustrative example is Twitter's home
timelines, which serve as a depiction of load, and response time percentiles
providing a metric for measuring performance. In a scalable system, the ability
to augment processing capacity becomes pivotal to sustaining reliability amidst
elevated loads.

The facet-rich concept of maintainability essentially revolves around enhancing
the working experience for engineering and operations teams interacting with the
system. Thoughtfully crafted abstractions play a crucial role in mitigating
complexity, rendering the system more adaptable and modifiable for emerging use
cases. Effective operability, characterized by comprehensive insights into the
system's health and adept management methods, also contributes to
maintainability.

Regrettably, there exists no panacea for achieving instant reliability,
scalability, or maintainability in applications. Nonetheless, discernible
patterns and recurring techniques emerge across diverse application types,
offering valuable insights into enhancing these critical attributes.

\subsubsection*{Data Transformation:}

Data Transformation is a vital part of the data flow process, where data changes
to meet specific requirements of the process or system. There are two main
directions for implementing data transformation: batch processing and real-time
processing.

In batch processing mode, data is processed in batches, often scheduled for
processing at predefined intervals. This is suitable for tasks requiring the
processing of large and complex datasets without an immediate response.

On the contrary, real-time processing is the method of processing data as soon
as it arrives, without waiting for a large amount of data to accumulate. This is
often preferred in applications demanding low latency, such as real-time event
processing.

\subsubsection*{ETL:}

ETL, an acronym for Extract, Transform, Load, stands as a fundamental
methodology indispensable for orchestrating the seamless movement of data within
storage ecosystems. This three-step process plays a pivotal role in shaping the
lifecycle of data.

Firstly, in the "Extract" phase, data is sourced from diverse origins, ranging
from databases to files and online services. This initial step lays the
groundwork by retrieving relevant information from the varied reservoirs of
data.

Subsequently, in the "Transform" phase, the extracted data undergoes a
metamorphosis to align with the specific requirements of the target system. This
transformative stage encompasses tasks such as data cleansing, format
conversions, and even the computation of novel indices, ensuring the data is
refined and tailored to suit its intended purpose.

Finally, the "Load" phase marks the culmination of the ETL process. At this
juncture, the meticulously transformed data finds its destination, being loaded
into the designated storage system. This storage system typically takes the form
of a data warehouse or data lake, serving as the repository for the refined and
purpose-adapted data. In essence, ETL encapsulates a systematic and
indispensable approach to managing the intricate journey of data within the
expansive realm of storage systems.

\subsubsection*{Data Pipe:}

A data pipe is an essential concept in deploying effective data flow. Built on
the idea of automating the movement and transformation of data, data pipes serve
as powerful workflow streams.

Through the use of data pipes, large data volumes can be processed flexibly and
efficiently. Tasks such as error handling, performance monitoring, and even
deploying new transformations can be automated, minimizing manual intervention
and enhancing system stability.


\subsubsection*{Data Orchestration:}

Data Orchestration is the intricate process of coordinating and managing
multiple data processes, workflows, or services to achieve specific outcomes. At
its core, it involves the meticulous definition and management of workflows,
determining the sequential order, and dependencies among various data processes.
Task execution is finely tuned through controllers that schedule and orchestrate
tasks at optimal times and in a precise sequence, ensuring the desired outcomes
are achieved. Controllers play a pivotal role in managing dependencies between
tasks, orchestrating the execution of tasks only when their dependent tasks are
successfully met. Robust orchestration systems provide comprehensive tools for
monitoring workflow progress and logging pertinent information, offering
insights to address and rectify any potential issues. Moreover, controllers
optimize performance by strategically breaking down complex tasks into multiple
subtasks, executing them in parallel to enhance overall system efficiency.